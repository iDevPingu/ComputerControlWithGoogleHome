{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # Colab only\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colab에서 실행 중이라면...\n",
    "# !git clone https://github.com/hukim1112/comment_classifier.git\n",
    "# import os\n",
    "# os.chdir('/content/comment_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "keras = tf.keras\n",
    "t = Okt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. fit tokenizer to our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorizer import BaseVectorizer\n",
    "vectorizer = BaseVectorizer(t.morphs)\n",
    "tokenizer = BaseVectorizer(t.morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Entitytest.csv',encoding='CP949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>빨간색</td>\n",
       "      <td>color</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word entity\n",
       "0  빨간색  color"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanning was done                                        \n",
      "105 terms are recognized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<vectorizer.BaseVectorizer at 0x1f185d287c8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.fit(df['word'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_PAD_': 0,\n",
       " '_UNK_': 1,\n",
       " '_STA_': 2,\n",
       " '_EOS_': 3,\n",
       " '색': 4,\n",
       " '보라색': 5,\n",
       " '공': 6,\n",
       " '과자': 7,\n",
       " '지갑': 8,\n",
       " '위': 9,\n",
       " '아래': 10,\n",
       " '맨': 11,\n",
       " '빨간색': 12,\n",
       " '파랑색': 13,\n",
       " '빨강': 14,\n",
       " '파란색': 15,\n",
       " '주황색': 16,\n",
       " '노란색': 17,\n",
       " '노랑': 18,\n",
       " '초록색': 19,\n",
       " '녹색': 20,\n",
       " '연두색': 21,\n",
       " '남색': 22,\n",
       " '자주색': 23,\n",
       " '하늘색': 24,\n",
       " '연': 25,\n",
       " '분홍색': 26,\n",
       " '청록색': 27,\n",
       " '청색': 28,\n",
       " '적색': 29,\n",
       " '갈색': 30,\n",
       " '네이비': 31,\n",
       " '자동차': 32,\n",
       " '축구공': 33,\n",
       " '농구공': 34,\n",
       " '피구': 35,\n",
       " '럭비공': 36,\n",
       " '야구공': 37,\n",
       " '테니스공': 38,\n",
       " '장난감': 39,\n",
       " '의자': 40,\n",
       " '책': 41,\n",
       " '책상': 42,\n",
       " '컵': 43,\n",
       " '상자': 44,\n",
       " '박스': 45,\n",
       " '게임기': 46,\n",
       " '컴퓨터': 47,\n",
       " '텔레비전': 48,\n",
       " '티비': 49,\n",
       " '가방': 50,\n",
       " '양말': 51,\n",
       " '패딩': 52,\n",
       " '외투': 53,\n",
       " '티셔츠': 54,\n",
       " '셔츠': 55,\n",
       " '바지': 56,\n",
       " '청바지': 57,\n",
       " '핸드폰': 58,\n",
       " '휴대폰': 59,\n",
       " '키': 60,\n",
       " '보드': 61,\n",
       " '마우스': 62,\n",
       " '모니터': 63,\n",
       " '노트북': 64,\n",
       " '공책': 65,\n",
       " '오른쪽': 66,\n",
       " '왼쪽': 67,\n",
       " '좌회전': 68,\n",
       " '우회': 69,\n",
       " '전': 70,\n",
       " '좌측': 71,\n",
       " '우측': 72,\n",
       " '저기': 73,\n",
       " '여기': 74,\n",
       " '저쪽': 75,\n",
       " '동쪽': 76,\n",
       " '서쪽': 77,\n",
       " '남쪽': 78,\n",
       " '북쪽': 79,\n",
       " '동': 80,\n",
       " '서': 81,\n",
       " '남': 82,\n",
       " '북': 83,\n",
       " '옆': 84,\n",
       " '안': 85,\n",
       " '붉': 86,\n",
       " '은색': 87,\n",
       " '푸른색': 88,\n",
       " '누런색': 89,\n",
       " '푸르스름': 90,\n",
       " '한': 91,\n",
       " '붉은': 92,\n",
       " '빨간': 93,\n",
       " '파란': 94,\n",
       " '초록': 95,\n",
       " '청록': 96,\n",
       " '우유': 97,\n",
       " '손목시계': 98,\n",
       " '시계': 99,\n",
       " '휴지': 100,\n",
       " '물티슈': 101,\n",
       " '커피포트': 102,\n",
       " '아이패드': 103,\n",
       " '아이폰': 104}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.char2idx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_PAD_': 0, '_UNK_': 1, '_': 2, 'P': 3, 'A': 4, 'D': 5, 'U': 6, 'N': 7, 'K': 8, 'S': 9, 'T': 10, 'E': 11, 'O': 12, '색': 13, '보': 14, '라': 15, '공': 16, '과': 17, '자': 18, '지': 19, '갑': 20, '위': 21, '아': 22, '래': 23, '맨': 24, '빨': 25, '간': 26, '파': 27, '랑': 28, '강': 29, '란': 30, '주': 31, '황': 32, '노': 33, '초': 34, '록': 35, '녹': 36, '연': 37, '두': 38, '남': 39, '하': 40, '늘': 41, '분': 42, '홍': 43, '청': 44, '적': 45, '갈': 46, '네': 47, '이': 48, '비': 49, '동': 50, '차': 51, '축': 52, '구': 53, '농': 54, '피': 55, '럭': 56, '야': 57, '테': 58, '니': 59, '스': 60, '장': 61, '난': 62, '감': 63, '의': 64, '책': 65, '상': 66, '컵': 67, '박': 68, '게': 69, '임': 70, '기': 71, '컴': 72, '퓨': 73, '터': 74, '텔': 75, '레': 76, '전': 77, '티': 78, '가': 79, '방': 80, '양': 81, '말': 82, '패': 83, '딩': 84, '외': 85, '투': 86, '셔': 87, '츠': 88, '바': 89, '핸': 90, '드': 91, '폰': 92, '휴': 93, '대': 94, '키': 95, '마': 96, '우': 97, '모': 98, '트': 99, '북': 100, '오': 101, '른': 102, '쪽': 103, '왼': 104, '좌': 105, '회': 106, '측': 107, '저': 108, '여': 109, '서': 110, '옆': 111, '안': 112, '붉': 113, '은': 114, '푸': 115, '누': 116, '런': 117, '르': 118, '름': 119, '한': 120, '유': 121, '손': 122, '목': 123, '시': 124, '계': 125, '물': 126, '슈': 127, '커': 128, '포': 129}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.char2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_id = {t:i for i,t in enumerate(df.entity.unique())}\n",
    "id_to_label = {i:t for i,t in enumerate(df.entity.unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'color': 0, 'thing': 1, 'loc': 2}\n"
     ]
    }
   ],
   "source": [
    "print(label_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.intent = df.intent.map(lambda x : label_index[x])\n",
    "# print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "def tokenize_and_filter(sentences, labels):\n",
    "    inputs, outputs = [], []\n",
    "  \n",
    "    for sentence, label in zip(sentences, labels):\n",
    "    # tokenize sentence\n",
    "        tokenized_sentence = tokenizer.encode_a_doc_to_list(sentence)\n",
    "        \n",
    "    # check tokenized sentence max length\n",
    "        if len(tokenized_sentence) <= MAX_LENGTH:\n",
    "            inputs.append(tokenized_sentence)\n",
    "#             print(\"input append\")\n",
    "            outputs.append(label_to_id[label])\n",
    "  \n",
    "  # pad tokenized sentences\n",
    "    padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        inputs, maxlen=MAX_LENGTH, padding='post', \n",
    "        value = tokenizer.vocabulary_['_PAD_']) # value = 0\n",
    "  \n",
    "    return padded_inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, outputs = tokenize_and_filter(df.word, df.entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded input :  [12  0  0  0  0  0  0  0  0  0] label :  0 original input sentence :  ['빨간색', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_', '_PAD_']\n"
     ]
    }
   ],
   "source": [
    "print('encoded input : ', inputs[0], 'label : ', outputs[0], 'original input sentence : ', tokenizer.decode_from_list(inputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "BUFFER_SIZE = 7836\n",
    "\n",
    "# decoder inputs use the previous target as input\n",
    "# remove START_TOKEN from targets\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, 10), (None,)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 34   0   0   0   0   0   0   0   0   0]\n",
      " [ 71   0   0   0   0   0   0   0   0   0]\n",
      " [ 38   0   0   0   0   0   0   0   0   0]\n",
      " [100   0   0   0   0   0   0   0   0   0]\n",
      " [ 67   0   0   0   0   0   0   0   0   0]\n",
      " [ 72   0   0   0   0   0   0   0   0   0]\n",
      " [ 73   0   0   0   0   0   0   0   0   0]\n",
      " [ 64   0   0   0   0   0   0   0   0   0]\n",
      " [ 81   0   0   0   0   0   0   0   0   0]\n",
      " [ 10   0   0   0   0   0   0   0   0   0]\n",
      " [103   0   0   0   0   0   0   0   0   0]\n",
      " [ 83   0   0   0   0   0   0   0   0   0]\n",
      " [ 66   0   0   0   0   0   0   0   0   0]\n",
      " [  7   0   0   0   0   0   0   0   0   0]\n",
      " [ 94   0   0   0   0   0   0   0   0   0]\n",
      " [  9   0   0   0   0   0   0   0   0   0]], shape=(16, 10), dtype=int32) tf.Tensor([1 2 1 1 2 2 2 1 2 2 1 2 2 1 0 2], shape=(16,), dtype=int32)\n",
      "-----------------------------------------------\n",
      "(16, 10) (16,)\n"
     ]
    }
   ],
   "source": [
    "for x, y in dataset.take(1):\n",
    "    print(x, y)\n",
    "    print('-----------------------------------------------')\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. model design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(label_to_id.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(tokenizer.n_vocabs, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(label_to_id.values()), activation='softmax')\n",
    "])\n",
    "    LEARNING_RATE = 0.001\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE),\n",
    "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE),\n",
    "#               loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "#               metrics=[tf.keras.metrics.sparse_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.8523 - sparse_categorical_accuracy: 0.5152\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.7719 - sparse_categorical_accuracy: 0.6061\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6148 - sparse_categorical_accuracy: 0.7677\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.4566 - sparse_categorical_accuracy: 0.7677\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.3392 - sparse_categorical_accuracy: 0.8283\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2612 - sparse_categorical_accuracy: 0.9596\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2213 - sparse_categorical_accuracy: 0.8788\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2093 - sparse_categorical_accuracy: 0.9192\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1230 - sparse_categorical_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0813 - sparse_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e207d64c88>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_processing(sentences):\n",
    "    MAX_LENGTH = 10\n",
    "    inputs = []\n",
    "    for sentence in sentences:\n",
    "        # tokenize sentence\n",
    "        tokenized_sentence = tokenizer.encode_a_doc_to_list(sentence)\n",
    "        # check tokenized sentence max length\n",
    "        if len(tokenized_sentence) <= MAX_LENGTH:\n",
    "            inputs.append(tokenized_sentence)\n",
    "        else:\n",
    "            print('입력이 너무 길어요.')\n",
    "    # pad tokenized sentences\n",
    "    padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    inputs, maxlen=MAX_LENGTH, padding='post', \n",
    "    value = tokenizer.vocabulary_['_PAD_']) # value = 0\n",
    "    return padded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = question_processing(['빨간색','휴지','파랑색','비','무지개색','오른쪽','노란색','유니폼','상자','옆','위'\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.5373678e-01, 8.5971355e-05, 4.6177287e-02],\n",
       "       [5.6485320e-08, 9.9999642e-01, 3.6223596e-06],\n",
       "       [9.3436652e-01, 1.3932100e-04, 6.5494195e-02],\n",
       "       [9.9562109e-04, 9.2088032e-01, 7.8124106e-02],\n",
       "       [9.4302708e-01, 1.3661891e-04, 5.6836307e-02],\n",
       "       [3.1226311e-02, 4.0562011e-02, 9.2821169e-01],\n",
       "       [8.8532829e-01, 3.3773883e-04, 1.1433393e-01],\n",
       "       [9.9562109e-04, 9.2088032e-01, 7.8124106e-02],\n",
       "       [9.6571647e-08, 9.9999440e-01, 5.5115884e-06],\n",
       "       [2.4339160e-02, 5.0067291e-02, 9.2559355e-01],\n",
       "       [2.3435676e-02, 6.0822763e-02, 9.1574150e-01]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 0 2 0 1 1 2 2]\n"
     ]
    }
   ],
   "source": [
    "prediction = np.argmax(model.predict(input_sentence), axis=1)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "color\n",
      "thing\n",
      "color\n",
      "thing\n",
      "color\n",
      "loc\n",
      "color\n",
      "thing\n",
      "thing\n",
      "loc\n",
      "loc\n"
     ]
    }
   ],
   "source": [
    "for p in prediction:\n",
    "    print(id_to_label[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 64)          6720      \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 81,219\n",
      "Trainable params: 81,219\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장하기\n",
    "model.save('entitytestmodel.h5')\n",
    "# 모델 불러오기\n",
    "tempmodel = keras.models.load_model('entitytestmodel.h5')\n",
    "tempmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.5373678e-01, 8.5971355e-05, 4.6177287e-02],\n",
       "       [5.6485320e-08, 9.9999642e-01, 3.6223596e-06],\n",
       "       [9.3436652e-01, 1.3932100e-04, 6.5494195e-02],\n",
       "       [9.9562109e-04, 9.2088032e-01, 7.8124106e-02],\n",
       "       [9.4302708e-01, 1.3661891e-04, 5.6836307e-02],\n",
       "       [3.1226311e-02, 4.0562011e-02, 9.2821169e-01],\n",
       "       [8.8532829e-01, 3.3773883e-04, 1.1433393e-01],\n",
       "       [9.9562109e-04, 9.2088032e-01, 7.8124106e-02],\n",
       "       [9.6571647e-08, 9.9999440e-01, 5.5115884e-06],\n",
       "       [2.4339160e-02, 5.0067291e-02, 9.2559355e-01],\n",
       "       [2.3435676e-02, 6.0822763e-02, 9.1574150e-01]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempmodel.predict(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 1, 0, 2, 0, 1, 1, 2, 2]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(np.argmax(model.predict(input_sentence), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "color\n",
      "thing\n",
      "color\n",
      "thing\n",
      "color\n",
      "loc\n",
      "color\n",
      "thing\n",
      "thing\n",
      "loc\n",
      "loc\n"
     ]
    }
   ],
   "source": [
    "for p in prediction:\n",
    "    print(id_to_label[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 추가해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['안중근', '이순신', '세종대왕', '김광석', '아이유', '에미넴', '이건희', '고아라', '유재석', '한석희', '최민성']\n",
    "def question_generator(names):\n",
    "    question = []\n",
    "    for name in names:\n",
    "        s1 = name+'는 어떤 분이야?'\n",
    "        s2 = name+'은 어떤 사람이니?'\n",
    "        s3 = name+'이란 사람에 대해 궁금해'\n",
    "        question = question+[s1, s2, s3]\n",
    "    return question\n",
    "question = question_generator(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {'question' : question, 'intent' : ['인물']*len(question)}\n",
    "add_df = pd.DataFrame(new_data, columns=('question', 'intent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df), len(add_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat([df, add_df])\n",
    "print(len(new_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit(new_df['question'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_inputs, new_outputs = tokenize_and_filter(new_df.question, new_df.intent)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "BUFFER_SIZE = 7836\n",
    "\n",
    "# decoder inputs use the previous target as input\n",
    "# remove START_TOKEN from targets\n",
    "dataset = tf.data.Dataset.from_tensor_slices((new_inputs, new_outputs))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = get_model()\n",
    "LEARNING_RATE = 0.0001\n",
    "new_model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE),\n",
    "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
    "new_model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = question_processing(['서울 날씨 어때?', \n",
    "                                      '나는 전주 날씨 궁금함',\n",
    "                                      '안중근 의사는 누구야?',\n",
    "                                      '박소희는 어떤 사람인지 궁금해.',\n",
    "                                      '명동 맛있는 음식점 있니?'\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.predict(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(new_model.predict(input_sentence), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in prediction:\n",
    "    print(id_to_label[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "from konlpy.utils import pprint\n",
    "kkma = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def 형태소분석(text):\n",
    "    형태소 = kkma.pos(text)\n",
    "    명사 = []\n",
    "    for i in 형태소:\n",
    "        if i[1] == 'NNG':\n",
    "            명사.append(i[0])\n",
    "        else:\n",
    "            pass\n",
    "    return 명사\n",
    "\n",
    "def entity분석(명사):\n",
    "    inputdata = question_processing(명사)\n",
    "    prediction = list(np.argmax(tempmodel.predict(inputdata),axis=1))\n",
    "    print(명사)\n",
    "    print(prediction)\n",
    "    result = {}\n",
    "    for a,b in zip(명사,prediction):\n",
    "        if b == 0:\n",
    "            result[a] = 'color'\n",
    "        elif b == 1:\n",
    "            result[a] = 'thing'\n",
    "        elif b == 2:\n",
    "            result[a] = 'loc'\n",
    "    return result  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['빨간색', '공책', '녹색', '상자', '안']\n",
      "[0, 1, 0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "tempmodel = keras.models.load_model('entitytestmodel.h5')\n",
    "noun = 형태소분석('빨간색 공책 녹색 상자 안에 넣어')\n",
    "result = entity분석(noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'빨간색': 'color', '공책': 'thing', '녹색': 'color', '상자': 'thing', '안': 'loc'}"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
