{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "from konlpy.utils import pprint\n",
    "kkma = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['테이블 위에 있는 빨간색 공 욕조로 옮겨 줘']\n"
     ]
    }
   ],
   "source": [
    "pprint(kkma.sentences(u'테이블위에있는 빨간색 공 욕조로 옮겨줘'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('테이블', 'NNG'),\n",
      " ('위', 'NNG'),\n",
      " ('에', 'JKM'),\n",
      " ('있', 'VV'),\n",
      " ('는', 'ETD'),\n",
      " ('빨간색', 'NNG'),\n",
      " ('공', 'NNG'),\n",
      " ('욕조', 'NNG'),\n",
      " ('로', 'JKM'),\n",
      " ('옮기', 'VV'),\n",
      " ('어', 'ECS'),\n",
      " ('주', 'VXV'),\n",
      " ('어', 'ECS')]\n"
     ]
    }
   ],
   "source": [
    "pprint(kkma.pos('테이블위에있는 빨간색 공 욕조로 옮겨줘'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"C:\\\\Users\\\\userew\\\\Desktop\\\\comment_classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['테이블', '테이블위', '위', '빨간색', '공', '욕조']\n"
     ]
    }
   ],
   "source": [
    "print(kkma.nouns(u'테이블위에있는 빨간색 공 욕조로 옮겨줘'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('파랗', 'VA'),\n",
      " ('ㄴ', 'ETD'),\n",
      " ('공', 'NNG'),\n",
      " ('테이블', 'NNG'),\n",
      " ('위', 'NNG'),\n",
      " ('에', 'JKM'),\n",
      " ('두', 'VV'),\n",
      " ('어', 'ECS')]\n"
     ]
    }
   ],
   "source": [
    "pprint(kkma.pos('파란공 테이블 위에 둬'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = kkma.pos('테이블위에있는 빨간색 공 욕조로 옮겨줘')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun = []\n",
    "for i in a:\n",
    "    if i[1] == 'NNG':\n",
    "        noun.append(i[0])\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['테이블', '위', '빨간색', '공', '욕조']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint, konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = konlpy.utils.read_json(\"simplejson.json\",encoding=u'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('안녕', 'NNG'),\n",
      " ('하', 'XSV'),\n",
      " ('세요', 'EFN'),\n",
      " ('저', 'NP'),\n",
      " ('는', 'JX'),\n",
      " ('차세대', 'NNG'),\n",
      " ('융합', 'NNG'),\n",
      " ('기술', 'NNG'),\n",
      " ('연구원', 'NNG'),\n",
      " ('에서', 'JKM'),\n",
      " ('인턴', 'NNG'),\n",
      " ('을', 'JKO'),\n",
      " ('수행', 'NNG'),\n",
      " ('하', 'XSV'),\n",
      " ('고', 'ECE'),\n",
      " ('있', 'VXV'),\n",
      " ('는', 'ETD'),\n",
      " ('황재익', 'UN'),\n",
      " ('이라고', 'JX'),\n",
      " ('하', 'VV'),\n",
      " ('ㅂ니다', 'EFN'),\n",
      " ('.', 'SF'),\n",
      " ('만나', 'VV'),\n",
      " ('서', 'ECD'),\n",
      " ('반갑', 'VA'),\n",
      " ('습니다', 'EFN'),\n",
      " ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "konlpy.utils.pprint(kkma.pos(a['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요 저는 차세대융합기술연구원에서 인턴을 수행하고 있는 황재익이라고 합니다. 만나서 반갑습니다.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 a\n",
      "1\n",
      "2 b\n",
      "2\n",
      "3 c\n",
      "3\n",
      "4 d\n",
      "4\n",
      "5 e\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3,4,5]\n",
    "b = ['a','b','c','d','e']\n",
    "for i,j in zip(a,b):\n",
    "    print(i,j)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {'아이':\"thing\",\"동쪽\":\"loc\"}\n",
    "key = result.keys()\n",
    "returntext = ''\n",
    "for i in key:\n",
    "    returntext += (i+' = '+result[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아이 = thing동쪽 = loc'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returntext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def 형태소분석(text):\n",
    "    형태소 = kkma.pos(text)\n",
    "    명사 = []\n",
    "    for i in 형태소:\n",
    "        if i[1] == 'NNG':\n",
    "            명사.append(i[0])\n",
    "        else:\n",
    "            pass\n",
    "    return 명사\n",
    "\n",
    "def entity분석(명사):\n",
    "    inputdata = question_processing(명사,entitytoken)\n",
    "    prediction = list(np.argmax(entitymodel.predict(inputdata),axis=1))\n",
    "    print(명사)\n",
    "    print(prediction)\n",
    "    result = {}\n",
    "    for a,b in zip(명사,prediction):\n",
    "        if b == 0:\n",
    "            result[a] = 'color'\n",
    "        elif b == 1:\n",
    "            result[a] = 'thing'\n",
    "        elif b == 2:\n",
    "            result[a] = 'loc'\n",
    "    return result  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['빨간색', '공책', '녹색', '상자', '안']\n",
      "[0, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "noun = 형태소분석('빨간색 공책 녹색 상자 안에 넣어')\n",
    "result = entity분석(noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # Colab only\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "keras = tf.keras\n",
    "t = Okt()\n",
    "from vectorizer import BaseVectorizer\n",
    "vectorizer = BaseVectorizer(t.morphs)\n",
    "tokenizer = BaseVectorizer(t.morphs)\n",
    "df = pd.read_csv('Entitytest.csv',encoding='CP949')\n",
    "tokenizer.fit(df['word'].values)\n",
    "label_to_id = {t:i for i,t in enumerate(df.entity.unique())}\n",
    "id_to_label = {i:t for i,t in enumerate(df.entity.unique())}\n",
    "MAX_LENGTH = 10\n",
    "def tokenize_and_filter(sentences, labels):\n",
    "    inputs, outputs = [], []\n",
    "  \n",
    "    for sentence, label in zip(sentences, labels):\n",
    "    # tokenize sentence\n",
    "        tokenized_sentence = tokenizer.encode_a_doc_to_list(sentence)\n",
    "        \n",
    "    # check tokenized sentence max length\n",
    "        if len(tokenized_sentence) <= MAX_LENGTH:\n",
    "            inputs.append(tokenized_sentence)\n",
    "#             print(\"input append\")\n",
    "            outputs.append(label_to_id[label])\n",
    "  \n",
    "  # pad tokenized sentences\n",
    "    padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        inputs, maxlen=MAX_LENGTH, padding='post', \n",
    "        value = tokenizer.vocabulary_['_PAD_']) # value = 0\n",
    "  \n",
    "    return padded_inputs, outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
